Overriding model.yaml nc=80 with nc=7
                 from  n    params  module                                  arguments
  0                -1  1         0  models.common.Silence                   []
  1                -1  1      1856  models.common.Conv                      [3, 64, 3, 2]
  2                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]
  3                -1  1    212864  models.common.RepNCSPELAN4              [128, 256, 128, 64, 1]
  4                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]
  5                -1  1    847616  models.common.RepNCSPELAN4              [256, 512, 256, 128, 1]
  6                -1  1   2360320  models.common.Conv                      [512, 512, 3, 2]
  7                -1  1   2857472  models.common.RepNCSPELAN4              [512, 512, 512, 256, 1]
  8                -1  1   2360320  models.common.Conv                      [512, 512, 3, 2]
  9                -1  1   2857472  models.common.RepNCSPELAN4              [512, 512, 512, 256, 1]
 10                -1  1    656896  models.common.SPPELAN                   [512, 512, 256]
 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']
 12           [-1, 7]  1         0  models.common.Concat                    [1]
 13                -1  1   3119616  models.common.RepNCSPELAN4              [1024, 512, 512, 256, 1]
 14                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']
 15           [-1, 5]  1         0  models.common.Concat                    [1]
 16                -1  1    912640  models.common.RepNCSPELAN4              [1024, 256, 256, 128, 1]
 17                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]
 18          [-1, 13]  1         0  models.common.Concat                    [1]
 19                -1  1   2988544  models.common.RepNCSPELAN4              [768, 512, 512, 256, 1]
 20                -1  1   2360320  models.common.Conv                      [512, 512, 3, 2]
 21          [-1, 10]  1         0  models.common.Concat                    [1]
 22                -1  1   3119616  models.common.RepNCSPELAN4              [1024, 512, 512, 256, 1]
 23                 5  1    131328  models.common.CBLinear                  [512, [256]]
 24                 7  1    393984  models.common.CBLinear                  [512, [256, 512]]
 25                 9  1    656640  models.common.CBLinear                  [512, [256, 512, 512]]
 26                 0  1      1856  models.common.Conv                      [3, 64, 3, 2]
 27                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]
 28                -1  1    212864  models.common.RepNCSPELAN4              [128, 256, 128, 64, 1]
 29                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]
 30  [23, 24, 25, -1]  1         0  models.common.CBFuse                    [[0, 0, 0]]
 31                -1  1    847616  models.common.RepNCSPELAN4              [256, 512, 256, 128, 1]
 32                -1  1   2360320  models.common.Conv                      [512, 512, 3, 2]
 33      [24, 25, -1]  1         0  models.common.CBFuse                    [[1, 1]]
 34                -1  1   2857472  models.common.RepNCSPELAN4              [512, 512, 512, 256, 1]
 35                -1  1   2360320  models.common.Conv                      [512, 512, 3, 2]
 36          [25, -1]  1         0  models.common.CBFuse                    [[2]]
 37                -1  1   2857472  models.common.RepNCSPELAN4              [512, 512, 512, 256, 1]
 38[31, 34, 37, 16, 19, 22]  1  21556682  models.yolo.DualDDetect                 [7, [512, 512, 512, 256, 512, 512]]
yolov9 summary: 930 layers, 60811082 parameters, 60811050 gradients, 266.2 GFLOPs
[34m[1mAMP: [39m[22mchecks passed âœ…
[34m[1moptimizer:[39m[22m SGD(lr=0.01) with parameter groups 230 weight(decay=0.0), 247 weight(decay=0.0005), 245 bias
[34m[1malbumentations: [39m[22mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))

[34m[1mtrain: [39m[22mScanning /mnt/d/Aquarium Combined.v2-raw-1024.yolov7pytorch/train/labels... 448 images, 1 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448/448 00:01
[34m[1mtrain: [39m[22mNew cache created: /mnt/d/Aquarium Combined.v2-raw-1024.yolov7pytorch/train/labels.cache
[34m[1mval: [39m[22mScanning /mnt/d/Aquarium Combined.v2-raw-1024.yolov7pytorch/valid/labels... 127 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 00:00
[34m[1mval: [39m[22mNew cache created: /mnt/d/Aquarium Combined.v2-raw-1024.yolov7pytorch/valid/labels.cache
Plotting labels to runs/train/exp2/labels.jpg...
Image sizes 640 train, 640 val
Using 8 dataloader workers
Logging results to [1mruns/train/exp2
Starting training for 5 epochs...
      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
  0%|          | 0/28 00:23
Traceback (most recent call last):
  File "/home/shawn/projects/yolov9/train.py", line 634, in <module>
    main(opt)
  File "/home/shawn/projects/yolov9/train.py", line 528, in main
    train(opt.hyp, opt, device, callbacks)
  File "/home/shawn/projects/yolov9/train.py", line 304, in train
    loss, loss_items = compute_loss(pred, targets.to(device))  # loss scaled by batch_size
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shawn/projects/yolov9/utils/loss_tal.py", line 168, in __call__
    pred_distri, pred_scores = torch.cat([xi.view(feats[0].shape[0], self.no, -1) for xi in feats], 2).split(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shawn/projects/yolov9/utils/loss_tal.py", line 168, in <listcomp>
    pred_distri, pred_scores = torch.cat([xi.view(feats[0].shape[0], self.no, -1) for xi in feats], 2).split(
                                          ^^^^^^^
AttributeError: 'list' object has no attribute 'view'